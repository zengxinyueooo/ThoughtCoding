# 当前上下文管理策略详解

## 📋 你的当前配置

根据你的 `config.yaml` 文件，当前配置如下：

```yaml
context:
  enabled: true                    # ✅ 已启用上下文管理
  strategy: "token_based"          # 🔥 使用 Token 控制策略
  maxHistoryTurns: 10              # 最多保留 10 轮对话
  maxContextTokens: 3000           # 历史最多占用 3000 tokens
  reserveTokens: 1000              # 为 AI 响应预留 1000 tokens
  showStatistics: true             # 显示统计信息
```

---

## 🎯 当前策略：Token 控制（TOKEN_BASED）

### 策略说明

**Token 控制策略**会根据实际的 Token 数量动态截断历史，而不是简单地按消息数量截断。

### 工作原理

```
完整对话历史（50条消息）
    ↓
ContextManager 逐条计算 Token
    ↓
从最新消息往前累加，直到达到 3000 tokens
    ↓
超过限制的旧消息被丢弃
    ↓
发送截断后的历史给 AI
```

### Token 估算方法

代码中使用的估算规则：
```java
// 中文字符：2 个字符 ≈ 1 token
// 英文字符：4 个字符 ≈ 1 token

示例：
"你好世界" → 4 个中文字符 → 约 2 tokens
"Hello World" → 11 个英文字符 → 约 2.75 tokens
"查看 pom.xml 文件" → 混合内容 → 约 6 tokens
```

---

## 📊 实际效果举例

### 场景 1：短对话（无截断）

**对话内容：**
```
第1轮:
User: 你好
AI: 你好！我是 ThoughtCoding，有什么可以帮你的？

第2轮:
User: 查看 pom.xml
AI: 好的，我来查看 pom.xml 文件... [200字回答]

第3轮:
User: 这个项目用了什么框架？
AI: 根据 pom.xml，项目使用了... [150字回答]
```

**Token 统计：**
```
第1轮: 约 50 tokens
第2轮: 约 150 tokens
第3轮: 约 120 tokens
总计: 约 320 tokens
```

**结果：✅ 完全不截断**
```
📊 上下文统计:
  完整历史: 6 条消息 (~320 tokens)
  发送历史: 6 条消息 (~320 tokens)
  节省: 0 tokens (0%)
  策略: token_based
```

所有历史都会发送给 AI，因为远低于 3000 tokens 限制。

---

### 场景 2：中等对话（部分截断）

**对话内容：**
```
第1轮: 关于项目架构的讨论 (约 400 tokens)
第2轮: 分析代码结构 (约 600 tokens)
第3轮: 讨论设计模式 (约 500 tokens)
第4轮: 代码审查建议 (约 700 tokens)
第5轮: 性能优化方案 (约 800 tokens)
第6轮: 部署流程讨论 (约 600 tokens)
第7轮: 当前问题... (约 200 tokens)
```

**Token 统计：**
```
总历史: 14 条消息，约 3800 tokens
限制: 3000 tokens
```

**截断过程：**
```
从最新消息开始倒序累加:
第7轮 (200) → 累计 200 ✅
第6轮 (600) → 累计 800 ✅
第5轮 (800) → 累计 1600 ✅
第4轮 (700) → 累计 2300 ✅
第3轮 (500) → 累计 2800 ✅
第2轮 (600) → 累计 3400 ❌ 超限！停止
第1轮 (400) → 被丢弃
```

**结果：⚠️ 截断前2轮**
```
📊 上下文统计:
  完整历史: 14 条消息 (~3800 tokens)
  发送历史: 10 条消息 (~2800 tokens)
  节省: 1000 tokens (26%)
  策略: token_based
```

**AI 看到的历史：**
```
第3轮: 讨论设计模式
第4轮: 代码审查建议
第5轮: 性能优化方案
第6轮: 部署流程讨论
第7轮: 当前问题
```

第1-2轮的内容被丢弃，AI 不知道最早的架构讨论和代码结构分析。

---

### 场景 3：超长对话（大量截断）

**对话内容：**
```
进行了 30 轮深入讨论，总计 60 条消息
总 Token: 约 8000 tokens
```

**截断过程：**
```
ContextManager 从最新开始累加:
- 保留最近约 15 条消息
- 丢弃前 45 条消息
```

**结果：⚠️ 大量截断**
```
📊 上下文统计:
  完整历史: 60 条消息 (~8000 tokens)
  发送历史: 15 条消息 (~2900 tokens)
  节省: 5100 tokens (64%)
  策略: token_based
```

**影响：**
- ✅ AI 能看到最近的上下文
- ❌ AI 完全不知道早期的讨论内容
- ⚠️ 如果早期有重要定义，AI 会"失忆"

---

## 🔄 与其他策略的对比

### 策略对比表

| 场景 | Token 控制策略 | 滑动窗口策略 | 混合策略 |
|------|---------------|-------------|---------|
| **场景1** (6条消息, 320 tokens) | 保留全部 6 条 | 保留全部 6 条 | 保留全部 6 条 |
| **场景2** (14条消息, 3800 tokens) | 保留 10 条 (2800 tokens) | 保留 10 条 (无论 tokens) | 先窗口再 Token |
| **场景3** (60条消息, 8000 tokens) | 保留 15 条 (2900 tokens) | 保留 20 条 (无论 tokens) | 保留 12 条 |

### 如果切换为滑动窗口策略

修改配置：
```yaml
context:
  strategy: "sliding_window"  # 改为滑动窗口
  maxHistoryTurns: 10         # 保留最近 10 轮
```

**场景2的效果：**
```
完整历史: 14 条消息 (7 轮对话)
    ↓
只保留最近 10 轮 = 20 条消息
    ↓
实际有 7 轮，全部保留
    ↓
发送全部 14 条消息 (3800 tokens)
    ↓
❌ 可能超过模型 maxTokens 限制！
```

**场景3的效果：**
```
完整历史: 60 条消息 (30 轮对话)
    ↓
只保留最近 10 轮 = 20 条消息
    ↓
发送 20 条消息 (约 5000 tokens)
    ↓
❌ 超过 3000 tokens 限制！可能被模型截断
```

---

## 💡 实际使用建议

### 当前配置（Token 控制）的优缺点

**优点：✅**
- ✅ 精确控制 Token 使用，不会超限
- ✅ 最大化利用可用的上下文窗口
- ✅ 自动适应不同长度的对话
- ✅ 保护系统不会因为历史过长而崩溃

**缺点：❌**
- ❌ 长对话后可能丢失重要的早期信息
- ❌ Token 估算不是完全精确（实际可能有 ±10% 误差）
- ❌ 用户不容易预测哪些历史会被保留

---

## 🎯 针对不同场景的建议

### 场景A：代码审查、文件分析（短期任务）

**推荐配置：**
```yaml
context:
  strategy: "token_based"
  maxContextTokens: 2000  # 更激进的限制
  maxHistoryTurns: 5
```

**原因：**
- 每个任务相对独立
- 不需要太长的历史
- 节省 Token，提升响应速度

---

### 场景B：故障分析、问题排查（需要完整上下文）

**推荐配置：**
```yaml
context:
  strategy: "token_based"
  maxContextTokens: 4000  # 更宽松的限制
  maxHistoryTurns: 15
```

**原因：**
- 需要记住之前的错误信息
- 需要关联多个线索
- 宁可多保留，避免"失忆"

---

### 场景C：日常对话、探索性讨论

**推荐配置（当前配置）：**
```yaml
context:
  strategy: "token_based"
  maxContextTokens: 3000
  maxHistoryTurns: 10
```

**原因：**
- 平衡性能和上下文
- 适合大多数场景
- ✅ 这是你当前的配置，已经很合理！

---

## 📈 如何观察实际效果

### 方法1：查看日志

运行项目时，ContextManager 会输出统计信息：

```bash
./bin/thought

# 在长对话后，你会看到：
📊 上下文统计:
  完整历史: 30 条消息 (~4200 tokens)
  发送历史: 18 条消息 (~2950 tokens)
  节省: 1250 tokens (30%)
  策略: token_based
```

### 方法2：测试对话

```bash
# 启动后进行长对话测试
User: 第1个问题...
AI: 回答...

User: 第2个问题...
AI: 回答...

# ... 重复 20 轮后 ...

User: 还记得我第1个问题吗？
AI: [如果被截断] 抱歉，我的上下文中没有最早的问题记录
    [如果未截断] 是的，你问的是...
```

---

## 🔧 调试技巧

### 临时查看完整统计

在 ContextManager.java 中，`logContextStatistics()` 方法会输出详细信息。

如果想看更详细的输出，可以修改日志级别：

```java
// ContextManager.java 的 logContextStatistics 方法
log.info("📊 上下文管理统计:");  // 改为 info 级别
log.info("  完整历史: {} 条消息 (~{} tokens)", fullHistory.size(), fullTokens);
log.info("  发送历史: {} 条消息 (~{} tokens)", managedHistory.size(), managedTokens);
```

---

## 🎓 总结

### 你当前的配置效果

```yaml
strategy: "token_based"
maxContextTokens: 3000
```

**意味着：**

1. **短对话（<3000 tokens）**：完全不截断，所有历史都保留 ✅
2. **中等对话（3000-5000 tokens）**：保留最近的消息，丢弃最早的 ⚠️
3. **长对话（>5000 tokens）**：大量截断，只保留最近约 50% 的内容 ❌

**适用场景：**
- ✅ 日常代码分析
- ✅ 文件操作
- ✅ 短期任务
- ⚠️ 长期故障排查（可能需要增加 maxContextTokens）
- ❌ 需要记住大量历史信息的任务

**这是一个非常合理的默认配置！** 既保护了系统不会因为历史过长而出问题，又尽可能保留了有用的上下文。

如果你发现 AI "失忆"太快，可以调大 `maxContextTokens` 到 4000 或 5000。

